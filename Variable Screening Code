import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

import seaborn as sns
import numpy as np
from scipy import stats
from scipy.stats import norm
from scipy.stats import skew

import warnings
warnings.filterwarnings('ignore')

df_train = pd.read_csv('C:/Users/arash/Desktop/561project/train.csv')
df_test=pd.read_csv('C:/Users/arash/Desktop/561project/test.csv')
##We have 1460 observations
df_train.head()
df_test.head()
#2.data processing

##Missing values
###checking which variables with #of NA>650
total = df_train.isnull().sum().sort_values(ascending=False)
percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
missing_data[missing_data.sum(axis=1) > (1460*0.15)]

###Delete features with NA>15%, meaningless features and highly corrleated to house price.
df_train.drop(['Id','Alley','FireplaceQu','LotFrontage','PoolQC', 'Fence', 'MiscFeature','Utilities', 'RoofMatl', 
                'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'Heating', 'LowQualFinSF',
               'BsmtFullBath', 'BsmtHalfBath', 'Functional',  'WoodDeckSF',
               'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea','MiscVal'],
              axis=1, inplace=True)

###deal with reamaining features having NA
for col in ('GarageType', 'GarageFinish', 'GarageQual','GarageCond','GarageYrBlt'):
    df_train[col] = df_train[col].fillna('NoGar')
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    df_train[col] = df_train[col].fillna('NoBsm')
####MasVnrType NA in all. filling with most popular values. 
df_train['MasVnrType'] = df_train['MasVnrType'].fillna(df_train['MasVnrType'].mode()[0])
#### filling with mean for MasVnrArea NAs. 
df_train['MasVnrArea'] = df_train['MasVnrArea'].fillna(df_train['MasVnrArea'].mean())
df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)

##Normality
###Check Normality
df_train['SalePrice'].describe()
sns.distplot(df_train['SalePrice'],axlabel="Original Price")

fig = plt.figure()
res = stats.probplot(df_train['SalePrice'], plot=plt)
plt.show()
"""from the graph, we can see the distribution of dependent variable is 
(1)not normal
(2)skew right
(3)has Kurtosis value
"""

###Do boxcox, log transform Y 
stats.boxcox(df_train['SalePrice']) 
##The result is close to zero, so do log tranform on Y
df_train['SalePrice'] = np.log1p(df_train['SalePrice'])
sns.distplot(df_train['SalePrice'],axlabel="log1p(price)")

fig = plt.figure()
res = stats.probplot(df_train['SalePrice'], plot=plt)
plt.show()
##no skewness
###log transform skewed numeric features:
numeric_feats = df_train.dtypes[df_train.dtypes != "object"].index
skewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness
skewed_feats = skewed_feats[skewed_feats > 0.75]
skewed_feats = skewed_feats.index
df_train[skewed_feats] = np.log1p(df_train[skewed_feats])


#3.multicollinearity &select variables 
##Apporach1: Correlation Matrix
matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)
corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)
##exists a lot strong multicollinearity
###10 features with the most effect on Y
k = 10 
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(df_train[cols].values.T)
sns.set(font_scale=1.5)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, 
                 yticklabels=cols.values, xticklabels=cols.values)
plt.show()

"""select factors
OverallQual
GrLiveArea(*) vs TotRmsAbvGrd
GarageCars
TotalBsmtSF(*) vs 1stFlrSF
FullBath
1stFlrSF(*) vs TotalBsmtSF
YearBuilt(*) vs YearRmodAdd"""

##Apporach2: Ridge
###convert categorical variable into dummy
y = df_train.SalePrice
df = df_train.drop('SalePrice', 1)
df = pd.get_dummies(df)


from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV
from sklearn.model_selection import cross_val_score

def rmse_cv(model):
    rmse= np.sqrt(-cross_val_score(model, df, y, scoring="neg_mean_squared_error", cv = 5))
    return(rmse)

model_ridge = Ridge()

alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]
cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() 
            for alpha in alphas]

cv_ridge = pd.Series(cv_ridge, index = alphas)
cv_ridge.plot(title = "Validation - Look for Optioptimized λ (tuning parameter)")
plt.xlabel("λ")
plt.ylabel("rmse")

cv_ridge.min()

##Approach3: lassso
model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(df, y)
rmse_cv(model_lasso).mean() ##rmse for Lasso method is smaller than Ridge. so we choose Lasso.

coef = pd.Series(model_lasso.coef_, index = df.columns)
print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")
##Lasso picked 101 variables and eliminated the other 238 variables

###the most important coefficients
imp_coef = pd.concat([coef.sort_values().tail(15)])    
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Lasso Model")
